\section{Experiments}
Further information regarding the train-validation-test split in our pipeline is provided below. \\\newline
Consider graph $G=(V, E)$, where $V$ denotes the set of vertices and $E$ is the set of edges.
A first split of the set of edges $E$ is needed for the embedding algorithms, creating $E_{\text{embed}}$ and $E_{\text{pred}}$.
$E_{\text{embed}}$ is used for the subgraph $G_{\text{embed}}=(V, E_{\text{embed}})$ given as input to the embedding methods:
the shallow embedding methods will use it to learn a lookup table $Z$ of node embeddings, whereas the deep embedding methods aim to learn a function $f$ that can generalize well. 
The remaining edges in $E_{\text{pred}}$ are instead reserved for the link-prediction task, where the second split occurs:
from $E_{\text{pred}}$ the sets $E_{\text{train}}$, $E_{\text{val}}$, $E_{\text{test}}$ are created for the training, validation and test of the prediction model. \\\newline
To train and evaluate the prediction models, labeled \emph{negative} edges are added to each of the three splits. 
A negative edge in this setting is defined to be an edge $(a,b) \notin E$ with $a,b \in V, a \ne b$ (do not include self-loops).
Furthermore, in order to avoid data leakage, the sampled negative edges must be distinct from those used internally by the embedding algorithms during training.\\\newline
For each edge $(u,v)$ belonging to any of these splits of $E_{\text{pred}}$, its corresponding embedded representation is constructed by combining the embeddings $z_u$, $z_v$ of its incident nodes, using an operator such as the average, the dot product, or concatenation. 
These edge embeddings are the ones used as input to the prediction models. \\\newline
A feasible split for a large graph could be: (from E) $80\%$ for $E_{\text{embed}}$ and $20\%$  for $E_{\text{pred}}$; (from $E_{\text{pred}}$) $60\%$ for $E_{\text{train}}$, $20\%$  for $E_{\text{val}}$ and $20\%$  for $E_{\text{test}}$.\\\newline
We revised the implementations that will be adopted for our experiments:  \cite{Node2VecImpl} for Node2Vec, \cite{LINEImpl} for LINE, \cite{DVNEImpl} for DVNE, \cite{GraphSageImpl} for GraphSage. 
Depending on specific needs, some of these could be modified or used as an inspiration for different implementations. 
In particular, DVNE was not evaluated on directed graphs in the original work \cite{DVNE}, meaning it may be necessary to explore adaptations for this setting.
If no satisfactory solution is found, DVNE will be restricted only for undirected graphs to mantain a fair comparison. \\ \newline 
Following some initial experiments, it was decided to use the DEI cluster \emph{"Blade"} as hardware for the computation of the larger graphs.
The specifications for the nodes of the cluster are available in the open documentation \cite{ClusterDEI}.
More information about the project's memory usage and runtime will be available in the final report. \\
