\section{Experiments}
We provide additional details on the train-validation-test split in our pipeline. 
Consider graph $G=(V, E)$, where $V$ denotes the set of vertices and $E$ is the set of edges.
A first split of the set of edges $E$ is needed for the embedding algorithms, creating $E_{\text{embed}}$ and $E_{\text{class}}$.
$E_{\text{embed}}$ is used for the subgraph $G_{\text{embed}}=(V, E_{\text{embed}})$ given as input to the embedding methods:
the shallow embedding methods will use it to learn a lookup table $Z$ of node embeddings, whereas the deep embedding methods aim to learn a function $f$ that can generalize
well. The remaining edges in $E_{\text{class}}$ are instead reserved for the classification task, where the second split occurs:
from $E_{\text{class}}$ the sets $E_{\text{train}}$, $E_{\text{val}}$, $E_{\text{test}}$ are created for the training, validation and test of the classification model. \\\newline
To train and evaluate the classification models, labeled \emph{negative} edges are added to each of the three splits. A negative edge in this setting is defined to be an edge $(a,b) \notin E$ with $a,b \in V$.
Furthermore, in order to avoid data leakage, the sampled negative edges must be distinct from those used internally by the embedding algorithms during training.\\\newline
For each edge $(u,v)$ belonging to any of these splits of $E_{\text{class}}$, its corresponding embedded representation is constructed by combining the embeddings $z_u$, $z_v$ of its incident nodes, using an operator such as the average, the dot product, or concatenation. 
These edge embeddings are the ones used as input to the classification models. \\\newline
A feasible split for a large graph could be: (from E) $80\%$ for $E_{\text{embed}}$ and $20\%$  for $E_{\text{class}}$; (from $E_{\text{class}}$) $60\%$  for $E_{\text{train}}$, $20\%$  for $E_{\text{val}}$ and $20\%$  for $E_{\text{test}}$.\\\newline
We revised the implementations that will be adopted for our experiments:  \cite{Node2VecImpl} for Node2Vec, \cite{LINEImpl} for LINE, \cite{DVNEImpl} for DVNE, \cite{GraphSageImpl} for GraphSage. 
Depending on our specific needs, some of these could be modified or used as an inspiration for our own implementations. 
In particular, DVNE was not evaluated on directed graphs in the original work \cite{DVNE}, meaning it may be necessary to explore adaptations for this setting.
If no satisfactory solution is found, DVNE will be restriced only for undirected graphs to mantain a fair comparison. \\ \newline 
Following some initial experiments, we decided to use the DEI cluster \emph{"Blade"} as hardware for the computation of the larger graphs.
The specifications for the nodes of the cluster are available in the open documentation \cite{ClusterDEI}.
More information about the project's memory usage and runtime will be available in the final report. \\
